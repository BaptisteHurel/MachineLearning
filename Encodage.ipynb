{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodage numérique de variables\n",
    "\n",
    "Tous les modèles que l'on utilise procède à de l'optimisation numérique. Seulement, pour effectuer des calculs numériques, ces variables doivent être transformés **numériquement**. Cela devient problématique lorsque l'on utilise des variables qualitatives et non ordinales (date, lieu, texte, ...). Heureusement, il existe diverses méthodes afin d'**encoder** ces variables de manière numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage par dictionnaire\n",
    "\n",
    "L'encodage par dictionnaire est fréquemment utilisé en NLP pour assimilier un mot particulier à un indice. Par exemple, si on considère la phrase *le chat adore le poisson*, alors la phrase pourra être encodée par *0 1 2 0 3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 0, 7, 8, 7, 4, 1, 6, 9, 7, 3, 0, 5], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [ \"le chat adore le poisson\", \"le chien aime la promenade\", \"le cheval adore galoper\" ]\n",
    "label_encoder = LabelEncoder()\n",
    "X_labels = label_encoder.fit_transform(\" \".join(X).split(\" \"))\n",
    "X_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas du texte, il existe beaucoup de méthodes pour encoder numériquement les variables : par fréquence ou codage, par TF-IDF, par dictionnaire ou encore par *word embedding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes : ['bleu' 'jaune' 'orange' 'rouge' 'vert']\n",
      "Encodage par labels : [0 3 4 3 1 2 0 4 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "X = [ \"bleu\", \"rouge\", \"vert\", \"rouge\", \"jaune\", \"orange\", \"bleu\", \"vert\", \"rouge\", \"rouge\", \"orange\" ]\n",
    "label_encoder = LabelEncoder()\n",
    "X_labels = label_encoder.fit_transform(\" \".join(X).split(\" \"))\n",
    "print(\"Classes :\", label_encoder.classes_)\n",
    "print(\"Encodage par labels :\", X_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage *one-hot*\n",
    "\n",
    "Un autre type d'encodage, particulièrement utilisé en NLP (traitement du langage) est l'encodage *one-hot*. Le principe est le suivant : on dispose d'une variable $X$ à $p$ modalités (pouvant prendre $p$ valeurs différentes) identifiées de $1$ à $p$. Pour représenter numériquement une observation de cette variable, on crée un vecteur de taille $p$ dont toutes les composantes sont nulles, sauf une composante qui vaut $1$ et qui correspond à la $i$⁻ème modalité.\n",
    "\n",
    "Par exemple, considérons quatre villes : Paris, Marseille, Nice et Bordeaux. On dispose d'une observation dont la valeur est Nice. Dans ce cas, le vecteur *one-hot* associé à cette observation est :\n",
    "\n",
    "$$(0 \\quad 0 \\quad 1 \\quad 0)$$\n",
    "\n",
    "Car Nice est bien la troisième modalité de la variable *ville*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes : ['Bordeaux' 'Marseille' 'Nice' 'Paris']\n",
      "Encodage par labels : [3 1 3 2 0 0 1]\n",
      "Encodage one-hot :\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bapti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray([ [\"Paris\"], [\"Marseille\"], [\"Paris\"], [\"Nice\"], [\"Bordeaux\"], [\"Bordeaux\"], [\"Marseille\"] ])\n",
    "# Création de l'objet Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "X_labels = label_encoder.fit_transform(X)\n",
    "print(\"Classes :\", label_encoder.classes_)\n",
    "print(\"Encodage par labels :\", X_labels)\n",
    "# Redimensionnement matriciel\n",
    "X_labels = X_labels.reshape(len(X_labels), 1)\n",
    "# Objet One Hot Encoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "print(\"Encodage one-hot :\")\n",
    "print(one_hot_encoder.fit_transform(X_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage transformé et circulaire\n",
    "\n",
    "Dans certains cas, même en présence de variables qualitatives, on souhaite obtenir un certain ordre lors du passage du qualitatif au numérique. Par exemple, on aimerait classer la météo selon plusieurs niveaux : proche de $0$ en cas de très mauvais temps, et proche de $1$ en cas de ciel bleu ensoleillé.\n",
    "\n",
    "Ce genre d'approche dépends entièrement de la problématique, de l'objectif de modélisation voir du modélisation en lui même.\n",
    "\n",
    "Autre exemple, supposons vouloir prédire la variation de la météo en fonction de la date : plutôt que de tronquer semaine par semaine ou jour par jour, on peut effectuer en encodage circulaire. On démarre de $0$ au mois de janvier pour arriver à $1$ en avril, puisque durant cette période, la météo varie beaucoup plus. Ensuite, on repasse à $0$ au mois juillet (en été, le temps reste globalement chaud) pour ensuite redescendre à $-1$ en octobre et terminer à $0$ en décembre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
